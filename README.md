# Credit Risk Analysis
module 17 challenge

## Goals
To analyze risk using machine learning algorithms and a dataset containing stats of previous loans. 

## Findings
### Sampling Tests
Within the dataset, there are 68470 cases of low-risk loans made and 347 high-risk loans. This is highly uneven, so we resampled the data using several different methods in order to produce more accurate predictions from the machine learning model. Using the oversampling, SMOTE, undersampling, and SMOTEEN methods we can compare accuracy scores to compare the efficacy of these various methods. To classify the risk in the dataset, we encoded the low risk and high risk to scores of 1 and 0 respectively. The accuracy score from testing random samples in the dataset is about the same across all four sampling methods with oversampling and SMOTE methods having the higher score of .72. The precision and sensitivity across all four methods of prediction are very good for predicting low risk applications at .99-1.00 (precision) and .69-.72(sensitivity/recall) across all four methods. The SMOTE method has the highest sensitivity (recall) and highest F1 score (.84) so it has the lowest disparity between precision and recall. Overall, the precision for high-risk applications is very low, but the sensitivity is not bad around .72-.73 for the resampling methods. The differences between the four sampling methods are very small across the confusion matrix and classification reports. For this type of testing, accuracy of the prediction is important when it comes to identifying good applicants for loans. Based on these four sampling methods, I think that the SMOTE resampling method has the highest precision and accuracy for low-risk applications and a higher sensitivity for high-risk applications as well making it the better method of the four.

### Ensemble Training
To improve our machine learning algorithm, we used a random forest classifier and an easy ensemble adaboost classifier to train the algorithm to improve predictions. Using a balanced random forest classifier, our accuracy score is .74 with good sensitivity and precision scores (.86 and 1.00 respectively). Also, using the balanced forest classifier method, the factors that bear the greatest importance in assessing risk in our feature importance ranking model are total_rec_prncp, last payment amount, total payment, and total payment invoice. This means these features have the greatest affect on risk outcome with total_rec_prncp having a feature importance score of .086. The AdaBoost classifier improved the accuracy of predictions in the test population significantly to an accuracy score of ~.93. The sensitivity is also boosted to .93 while the precision remains very high at 1.00. This decreased disparity also results in a higher f1 score (.96). This test greatly improves the accuracy and sensitivty of the machine learning model and is recommended for use in predicting risk for future applicants based on the applicant data set we used. 